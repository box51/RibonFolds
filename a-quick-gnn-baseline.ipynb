{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "467ac5d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-25T11:02:15.90837Z",
     "iopub.status.busy": "2023-09-25T11:02:15.908105Z"
    },
    "papermill": {
     "duration": 0.008882,
     "end_time": "2023-11-03T23:42:20.559416",
     "exception": false,
     "start_time": "2023-11-03T23:42:20.550534",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A quick GNN baseline\n",
    "\n",
    "In this notebook, I put together a very basic GNN starter. \n",
    "None of the below in necessarily optimal (it's basically what I was able to throw together on a Sunday afternoon), but should be enough to get started for further experimentation. I wrote this on my PC, so this notebook seems to be a little bit bottlenecked by the low amount of CPU cores, but I'm sure it's possible to work around that. The below doesn't actually do very well, but most of the work was the dataloader. Maybe you can do better? \n",
    "\n",
    "## Graph neural networks \n",
    "\n",
    "[Graph neural networks](https://en.wikipedia.org/wiki/Graph_neural_network) (GNNs) are a class of neural networks that operate on graph data structures. Graphs can be used to describe many objects, such as social networks, road networks, or in this case, molecules. \n",
    "\n",
    "Unlike sequences and grids (images) which have strict definitions of adjacency, graphs have a more flexible definition of adjacency, that is defined by the edges of the graph. \n",
    "\n",
    "## RNA as graphs\n",
    "\n",
    "There is no canonical way to represent an RNA molecule as a graph, and I believe the \"trick\" will be constructing the adjacency. In the case below, the base pairs are simply connected to the neares nodes (defined by `EDGE_DISTANCE`) on each side. If we had the 3D structure, that could be a good value to use for connections as well. From the info we do have, the pairing probablility might also work. \n",
    "\n",
    "# PyG\n",
    "\n",
    "[PyG](https://pyg.org/) (AKA pytorch geometric) is a library that contains all you need to get going with learning on graphs. \n",
    "\n",
    "Below I've used the PyG dataset class to create a dataset, and imported a simple GNN, called EdgeCNN, to begin with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2045583",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T23:42:20.577001Z",
     "iopub.status.busy": "2023-11-03T23:42:20.576644Z",
     "iopub.status.idle": "2023-11-03T23:42:34.187702Z",
     "shell.execute_reply": "2023-11-03T23:42:34.186603Z"
    },
    "papermill": {
     "duration": 13.622528,
     "end_time": "2023-11-03T23:42:34.190209",
     "exception": false,
     "start_time": "2023-11-03T23:42:20.567681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\r\n",
      "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (4.66.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.23.5)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.11.2)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.2)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2.31.0)\r\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.0.9)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.2.2)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (5.9.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch_geometric) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (2023.7.22)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (3.1.0)\r\n",
      "Installing collected packages: torch_geometric\r\n",
      "Successfully installed torch_geometric-2.4.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "480f7362",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T23:42:34.209223Z",
     "iopub.status.busy": "2023-11-03T23:42:34.208668Z",
     "iopub.status.idle": "2023-11-03T23:42:41.817421Z",
     "shell.execute_reply": "2023-11-03T23:42:41.816596Z"
    },
    "papermill": {
     "duration": 7.620644,
     "end_time": "2023-11-03T23:42:41.819738",
     "exception": false,
     "start_time": "2023-11-03T23:42:34.199094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import polars as pl\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46f80c02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T23:42:41.838738Z",
     "iopub.status.busy": "2023-11-03T23:42:41.837969Z",
     "iopub.status.idle": "2023-11-03T23:42:41.842742Z",
     "shell.execute_reply": "2023-11-03T23:42:41.841910Z"
    },
    "papermill": {
     "duration": 0.015892,
     "end_time": "2023-11-03T23:42:41.844527",
     "exception": false,
     "start_time": "2023-11-03T23:42:41.828635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"/kaggle/input/stanford-ribonanza-rna-folding/\")\n",
    "TRAIN_CSV = DATA_DIR / \"train_data.csv\"\n",
    "TRAIN_PARQUET_FILE = \"train_data.parquet\"\n",
    "TEST_CSV = DATA_DIR / \"test_sequences.csv\"\n",
    "TEST_PARQUET_FILE = \"test_sequences.parquet\"\n",
    "PRED_CSV = \"submission.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e8a4a",
   "metadata": {
    "papermill": {
     "duration": 0.008145,
     "end_time": "2023-11-03T23:42:41.861079",
     "exception": false,
     "start_time": "2023-11-03T23:42:41.852934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Convert to Parquet\n",
    "\n",
    "Pandas is too slow, so the below converts the training and testing data to parquet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99bf1cb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T23:42:41.878972Z",
     "iopub.status.busy": "2023-11-03T23:42:41.878660Z",
     "iopub.status.idle": "2023-11-03T23:42:41.884265Z",
     "shell.execute_reply": "2023-11-03T23:42:41.883427Z"
    },
    "papermill": {
     "duration": 0.016678,
     "end_time": "2023-11-03T23:42:41.886150",
     "exception": false,
     "start_time": "2023-11-03T23:42:41.869472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_parquet(csv_file, parquet_file):\n",
    "    dummy_df = pl.scan_csv(csv_file)\n",
    "\n",
    "    new_schema = {}\n",
    "    for key, value in dummy_df.schema.items():\n",
    "        if key.startswith(\"reactivity\"):\n",
    "            new_schema[key] = pl.Float32\n",
    "        else:\n",
    "            new_schema[key] = value\n",
    "\n",
    "    df = pl.scan_csv(csv_file, schema=new_schema)\n",
    "    \n",
    "    df.sink_parquet(\n",
    "            parquet_file,\n",
    "            compression='uncompressed',\n",
    "            row_group_size=10,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "268ed2be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T23:42:41.904290Z",
     "iopub.status.busy": "2023-11-03T23:42:41.903576Z",
     "iopub.status.idle": "2023-11-03T23:45:04.386477Z",
     "shell.execute_reply": "2023-11-03T23:45:04.385578Z"
    },
    "papermill": {
     "duration": 142.494364,
     "end_time": "2023-11-03T23:45:04.388910",
     "exception": false,
     "start_time": "2023-11-03T23:42:41.894546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_parquet(TRAIN_CSV, TRAIN_PARQUET_FILE)\n",
    "to_parquet(TEST_CSV, TEST_PARQUET_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74277e36",
   "metadata": {
    "papermill": {
     "duration": 0.008339,
     "end_time": "2023-11-03T23:45:04.406477",
     "exception": false,
     "start_time": "2023-11-03T23:45:04.398138",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define adjacency\n",
    "\n",
    "There are countless ways you can define adjacency. \n",
    "The function below creates an edge connection array that specifies how the edges are connected. \n",
    "\n",
    "In this case, an edge is connected to the neighbouring `n` molecules to each side of it, plus optionally itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14580721",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T23:45:04.424604Z",
     "iopub.status.busy": "2023-11-03T23:45:04.424325Z",
     "iopub.status.idle": "2023-11-03T23:45:04.431691Z",
     "shell.execute_reply": "2023-11-03T23:45:04.430826Z"
    },
    "papermill": {
     "duration": 0.018834,
     "end_time": "2023-11-03T23:45:04.433764",
     "exception": false,
     "start_time": "2023-11-03T23:45:04.414930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nearest_adjacency(sequence_length, n=2, loops=True):\n",
    "    base = np.arange(sequence_length)\n",
    "    connections = []\n",
    "    for i in range(-n, n + 1):\n",
    "        if i == 0 and not loops:\n",
    "            continue\n",
    "        elif i == 0 and loops:\n",
    "            stack = np.vstack([base, base])\n",
    "            connections.append(stack)\n",
    "            continue\n",
    "\n",
    "        neighbours = base.take(range(i,sequence_length+i), mode='wrap')\n",
    "        stack = np.vstack([base, neighbours])\n",
    "        \n",
    "        if i < 0:\n",
    "            connections.append(stack[:, -i:])\n",
    "        elif i > 0:\n",
    "            connections.append(stack[:, :-i])\n",
    "\n",
    "    return np.hstack(connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04056420",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T23:45:04.451970Z",
     "iopub.status.busy": "2023-11-03T23:45:04.451713Z",
     "iopub.status.idle": "2023-11-03T23:45:04.455516Z",
     "shell.execute_reply": "2023-11-03T23:45:04.454659Z"
    },
    "papermill": {
     "duration": 0.014747,
     "end_time": "2023-11-03T23:45:04.457346",
     "exception": false,
     "start_time": "2023-11-03T23:45:04.442599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EDGE_DISTANCE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c616564",
   "metadata": {
    "papermill": {
     "duration": 0.008217,
     "end_time": "2023-11-03T23:45:04.473924",
     "exception": false,
     "start_time": "2023-11-03T23:45:04.465707",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Defining the dataloader. \n",
    "\n",
    "The below defines a simple dataloader that parses a parquet file into node embeddings (for now just the one hot encoded bases A, G, U and C), the adjacency (using the function above), and the targets (the reactivity scores). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bd8d97c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T23:45:04.492167Z",
     "iopub.status.busy": "2023-11-03T23:45:04.491895Z",
     "iopub.status.idle": "2023-11-03T23:45:04.504433Z",
     "shell.execute_reply": "2023-11-03T23:45:04.503564Z"
    },
    "papermill": {
     "duration": 0.023916,
     "end_time": "2023-11-03T23:45:04.506300",
     "exception": false,
     "start_time": "2023-11-03T23:45:04.482384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleGraphDataset(Dataset):\n",
    "    def __init__(self, parquet_name, edge_distance=5, root=None, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        # Set csv name\n",
    "        self.parquet_name = parquet_name\n",
    "        # Set edge distance\n",
    "        self.edge_distance = edge_distance\n",
    "        # Initialize one hot encoder\n",
    "        self.node_encoder = OneHotEncoder(sparse_output=False, max_categories=5)\n",
    "        # For one-hot encoder to possible values\n",
    "        self.node_encoder.fit(np.array(['A', 'G', 'U', 'C']).reshape(-1, 1))\n",
    "        # Load dataframe\n",
    "        self.df = pl.read_parquet(self.parquet_name)\n",
    "        self.df = self.df.filter(pl.col(\"SN_filter\") == 1.0)\n",
    "        # Get reactivity columns names\n",
    "        reactivity_match = re.compile('(reactivity_[0-9])')\n",
    "        reactivity_names = [col for col in self.df.columns if reactivity_match.match(col)]\n",
    "        \n",
    "        self.reactivity_df = self.df.select(reactivity_names) \n",
    "\n",
    "        self.sequence_df = self.df.select(\"sequence\")\n",
    "\n",
    "    def parse_row(self, idx):\n",
    "        # Read row at idx\n",
    "        sequence_row = self.sequence_df.row(idx)  \n",
    "        reactivity_row = self.reactivity_df.row(idx)\n",
    "        # Get sequence string and convert to array\n",
    "        sequence = np.array(list(sequence_row[0])).reshape(-1, 1)\n",
    "        # Encode sequence array\n",
    "        encoded_sequence = self.node_encoder.transform(sequence)\n",
    "        # Get sequence length\n",
    "        sequence_length = len(sequence)\n",
    "        # Get edge index \n",
    "        edges_np = nearest_adjacency(sequence_length, n=self.edge_distance, loops=False)\n",
    "        # Convert to torch tensor\n",
    "        edge_index = torch.tensor(edges_np, dtype=torch.long)\n",
    "\n",
    "        # Get reactivity targets for nodes\n",
    "        reactivity = np.array(reactivity_row, dtype=np.float32)[0:sequence_length]\n",
    "     \n",
    "        # Create valid masks for nodes\n",
    "        valid_mask = np.argwhere(~np.isnan(reactivity)).reshape(-1)\n",
    "        torch_valid_mask = torch.tensor(valid_mask, dtype=torch.long)\n",
    "\n",
    "        # Replace nan values for reactivity with 0. \n",
    "        # Not actually super important as they get masked\n",
    "        reactivity = np.nan_to_num(reactivity, copy=False, nan=0.0)\n",
    "\n",
    "\n",
    "        # Define node features as one-hot encoded sequence\n",
    "        node_features = torch.Tensor(encoded_sequence)\n",
    "\n",
    "        # Targets \n",
    "        targets = torch.Tensor(reactivity)\n",
    "\n",
    "        data = Data(x=node_features, edge_index=edge_index, y=targets, valid_mask=torch_valid_mask)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = self.parse_row(idx) \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2373ba29",
   "metadata": {
    "papermill": {
     "duration": 0.008254,
     "end_time": "2023-11-03T23:45:04.523082",
     "exception": false,
     "start_time": "2023-11-03T23:45:04.514828",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data handling\n",
    "\n",
    "Define the train and validation datasets, and load them with dataloaders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47a9cc76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T23:45:04.541023Z",
     "iopub.status.busy": "2023-11-03T23:45:04.540733Z",
     "iopub.status.idle": "2023-11-03T23:45:13.178044Z",
     "shell.execute_reply": "2023-11-03T23:45:13.177012Z"
    },
    "papermill": {
     "duration": 8.64922,
     "end_time": "2023-11-03T23:45:13.180598",
     "exception": false,
     "start_time": "2023-11-03T23:45:04.531378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_train_dataset = SimpleGraphDataset(parquet_name=TRAIN_PARQUET_FILE, edge_distance=EDGE_DISTANCE)\n",
    "generator1 = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [0.7, 0.3], generator1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff4e62a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T23:45:13.199869Z",
     "iopub.status.busy": "2023-11-03T23:45:13.199072Z",
     "iopub.status.idle": "2023-11-03T23:45:13.205129Z",
     "shell.execute_reply": "2023-11-03T23:45:13.204196Z"
    },
    "papermill": {
     "duration": 0.017645,
     "end_time": "2023-11-03T23:45:13.207052",
     "exception": false,
     "start_time": "2023-11-03T23:45:13.189407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=2)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df3d6b8",
   "metadata": {
    "papermill": {
     "duration": 0.00842,
     "end_time": "2023-11-03T23:45:13.224196",
     "exception": false,
     "start_time": "2023-11-03T23:45:13.215776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loss and metrics\n",
    "\n",
    "Define the loss function and the MSE, and define the MAE as a loss. \n",
    "\n",
    "The target values are clipped to `(0, 1)` as the competition metric is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9b28caa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T23:45:13.242428Z",
     "iopub.status.busy": "2023-11-03T23:45:13.242141Z",
     "iopub.status.idle": "2023-11-03T23:45:13.247954Z",
     "shell.execute_reply": "2023-11-03T23:45:13.247057Z"
    },
    "papermill": {
     "duration": 0.01729,
     "end_time": "2023-11-03T23:45:13.249890",
     "exception": false,
     "start_time": "2023-11-03T23:45:13.232600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def loss_fn(output, target):\n",
    "    clipped_target = torch.clip(target, min=0, max=1)\n",
    "    mses = F.mse_loss(output, clipped_target, reduction='mean')\n",
    "    return mses\n",
    "\n",
    "def mae_fn(output, target):\n",
    "    clipped_target = torch.clip(target, min=0, max=1)\n",
    "    maes = F.l1_loss(output, clipped_target, reduction='mean')\n",
    "    return maes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41745f35",
   "metadata": {
    "papermill": {
     "duration": 0.008223,
     "end_time": "2023-11-03T23:45:13.266635",
     "exception": false,
     "start_time": "2023-11-03T23:45:13.258412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define the model\n",
    "\n",
    "Below we define as simple EdgeCNN from PyG. \n",
    "As a start, we give it 128 hidden channels, and 4 layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c98fb54d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T23:45:13.285365Z",
     "iopub.status.busy": "2023-11-03T23:45:13.284790Z",
     "iopub.status.idle": "2023-11-03T23:45:16.594631Z",
     "shell.execute_reply": "2023-11-03T23:45:16.593569Z"
    },
    "papermill": {
     "duration": 3.32152,
     "end_time": "2023-11-03T23:45:16.597045",
     "exception": false,
     "start_time": "2023-11-03T23:45:13.275525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn.models import EdgeCNN\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = EdgeCNN(in_channels=full_train_dataset.num_features, hidden_channels=128,\n",
    "                num_layers=4, out_channels=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "604578f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T23:45:16.615556Z",
     "iopub.status.busy": "2023-11-03T23:45:16.615247Z",
     "iopub.status.idle": "2023-11-03T23:45:16.621598Z",
     "shell.execute_reply": "2023-11-03T23:45:16.620734Z"
    },
    "papermill": {
     "duration": 0.017773,
     "end_time": "2023-11-03T23:45:16.623512",
     "exception": false,
     "start_time": "2023-11-03T23:45:16.605739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure we are using the GPU\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6add76cb",
   "metadata": {
    "papermill": {
     "duration": 0.008367,
     "end_time": "2023-11-03T23:45:16.640345",
     "exception": false,
     "start_time": "2023-11-03T23:45:16.631978",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training \n",
    "\n",
    "Train the model for 10 epochs. \n",
    "Is this a good learning rate? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "230ce58e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T23:45:16.658501Z",
     "iopub.status.busy": "2023-11-03T23:45:16.658226Z",
     "iopub.status.idle": "2023-11-04T00:44:35.734250Z",
     "shell.execute_reply": "2023-11-04T00:44:35.733040Z"
    },
    "papermill": {
     "duration": 3559.08842,
     "end_time": "2023-11-04T00:44:35.737304",
     "exception": false,
     "start_time": "2023-11-03T23:45:16.648884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1199 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch_geometric/warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "Train loss 0.1383: 100%|██████████| 1199/1199 [04:11<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss:  0.12228444\n",
      "Epoch 0 train mae:  0.2976345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss 0.1193: 100%|██████████| 514/514 [01:40<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 val loss:  0.12097534\n",
      "Epoch 0 val mae:  0.294571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss 0.1241: 100%|██████████| 1199/1199 [04:10<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss:  0.12205741\n",
      "Epoch 1 train mae:  0.29503754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss 0.1192: 100%|██████████| 514/514 [01:40<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 val loss:  0.12094678\n",
      "Epoch 1 val mae:  0.29391912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss 0.1468: 100%|██████████| 1199/1199 [04:13<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train loss:  0.121927805\n",
      "Epoch 2 train mae:  0.29464436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss 0.1192: 100%|██████████| 514/514 [01:44<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 val loss:  0.1209518\n",
      "Epoch 2 val mae:  0.29405153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss 0.1046: 100%|██████████| 1199/1199 [04:15<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train loss:  0.12191963\n",
      "Epoch 3 train mae:  0.29496667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss 0.1190: 100%|██████████| 514/514 [01:44<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 val loss:  0.12092747\n",
      "Epoch 3 val mae:  0.2932423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss 0.1411: 100%|██████████| 1199/1199 [04:16<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train loss:  0.122150816\n",
      "Epoch 4 train mae:  0.2945868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss 0.1193: 100%|██████████| 514/514 [01:42<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 val loss:  0.12096829\n",
      "Epoch 4 val mae:  0.2944289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss 0.1469: 100%|██████████| 1199/1199 [04:14<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train loss:  0.12214798\n",
      "Epoch 5 train mae:  0.29503128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss 0.1193: 100%|██████████| 514/514 [01:42<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 val loss:  0.120979026\n",
      "Epoch 5 val mae:  0.2946417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss 0.0975: 100%|██████████| 1199/1199 [04:14<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train loss:  0.12199342\n",
      "Epoch 6 train mae:  0.29454237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss 0.1193: 100%|██████████| 514/514 [01:42<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 val loss:  0.12099274\n",
      "Epoch 6 val mae:  0.29488832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss 0.1173: 100%|██████████| 1199/1199 [04:12<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train loss:  0.1211497\n",
      "Epoch 7 train mae:  0.29340014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss 0.1191: 100%|██████████| 514/514 [01:42<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 val loss:  0.12093639\n",
      "Epoch 7 val mae:  0.29360175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss 0.1181: 100%|██████████| 1199/1199 [04:14<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train loss:  0.12180431\n",
      "Epoch 8 train mae:  0.29447123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss 0.1192: 100%|██████████| 514/514 [01:42<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 val loss:  0.12095167\n",
      "Epoch 8 val mae:  0.2940483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss 0.1381: 100%|██████████| 1199/1199 [04:11<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 train loss:  0.121976785\n",
      "Epoch 9 train mae:  0.29442555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss 0.1192: 100%|██████████| 514/514 [01:40<00:00,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 val loss:  0.1209598\n",
      "Epoch 9 val mae:  0.29424408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_losses = []\n",
    "    train_maes = []\n",
    "    model.train()\n",
    "    for batch in (pbar := tqdm(train_dataloader, position=0, leave=True)):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        out = torch.squeeze(out)\n",
    "        loss = loss_fn(out[batch.valid_mask], batch.y[batch.valid_mask])\n",
    "        mae = mae_fn(out[batch.valid_mask], batch.y[batch.valid_mask])\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.detach().cpu().numpy())\n",
    "        train_maes.append(mae.detach().cpu().numpy())\n",
    "        optimizer.step()\n",
    "        pbar.set_description(f\"Train loss {loss.detach().cpu().numpy():.4f}\")       \n",
    "\n",
    "    print(f\"Epoch {epoch} train loss: \", np.mean(train_losses))    \n",
    "    print(f\"Epoch {epoch} train mae: \", np.mean(train_maes))    \n",
    "    \n",
    "    val_losses = []\n",
    "    val_maes = []\n",
    "    model.eval()\n",
    "    for batch in (pbar := tqdm(val_dataloader, position=0, leave=True)):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        out = torch.squeeze(out)\n",
    "        loss = loss_fn(out[batch.valid_mask], batch.y[batch.valid_mask])\n",
    "        mae = mae_fn(out[batch.valid_mask], batch.y[batch.valid_mask])\n",
    "        val_losses.append(loss.detach().cpu().numpy())\n",
    "        val_maes.append(mae.detach().cpu().numpy())\n",
    "        pbar.set_description(f\"Validation loss {loss.detach().cpu().numpy():.4f}\")      \n",
    "        \n",
    "    print(f\"Epoch {epoch} val loss: \", np.mean(val_losses)) \n",
    "    print(f\"Epoch {epoch} val mae: \", np.mean(val_maes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3353211",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T00:44:40.835103Z",
     "iopub.status.busy": "2023-11-04T00:44:40.834740Z",
     "iopub.status.idle": "2023-11-04T00:44:40.918356Z",
     "shell.execute_reply": "2023-11-04T00:44:40.917403Z"
    },
    "papermill": {
     "duration": 2.634055,
     "end_time": "2023-11-04T00:44:40.920374",
     "exception": false,
     "start_time": "2023-11-04T00:44:38.286319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529e9327",
   "metadata": {
    "papermill": {
     "duration": 2.560056,
     "end_time": "2023-11-04T00:44:46.040808",
     "exception": false,
     "start_time": "2023-11-04T00:44:43.480752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference on the test set\n",
    "\n",
    "Here we define a light weight dataset to handle the inference step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1084895a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T00:44:51.113550Z",
     "iopub.status.busy": "2023-11-04T00:44:51.113182Z",
     "iopub.status.idle": "2023-11-04T00:44:51.124337Z",
     "shell.execute_reply": "2023-11-04T00:44:51.123458Z"
    },
    "papermill": {
     "duration": 2.551408,
     "end_time": "2023-11-04T00:44:51.126742",
     "exception": false,
     "start_time": "2023-11-04T00:44:48.575334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InferenceGraphDataset(Dataset):\n",
    "    def __init__(self, parquet_name, edge_distance=2, root=None, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        # Set csv name\n",
    "        self.parquet_name = parquet_name\n",
    "        # Set edge distance\n",
    "        self.edge_distance = edge_distance\n",
    "        # Initialize one hot encoder\n",
    "        self.node_encoder = OneHotEncoder(sparse_output=False, max_categories=4)\n",
    "        # For one-hot encoder to possible values\n",
    "        self.node_encoder.fit(np.array(['A', 'G', 'U', 'C']).reshape(-1, 1))\n",
    "        # Load dataframe\n",
    "        self.df = pl.read_parquet(self.parquet_name)\n",
    "\n",
    "        self.sequence_df = self.df.select(\"sequence\")\n",
    "        self.id_min_df = self.df.select(\"id_min\")\n",
    "\n",
    "    def parse_row(self, idx):\n",
    "        # Read row at idx\n",
    "        sequence_row = self.sequence_df.row(idx)  \n",
    "        id_min = self.id_min_df.row(idx)[0]\n",
    "\n",
    "        # Get sequence string and convert to array\n",
    "        sequence = np.array(list(sequence_row[0])).reshape(-1, 1)\n",
    "        # Encode sequence array\n",
    "        encoded_sequence = self.node_encoder.transform(sequence)\n",
    "        # Get sequence length\n",
    "        sequence_length = len(sequence)\n",
    "        # Get edge index \n",
    "        edges_np = nearest_adjacency(sequence_length, n=self.edge_distance, loops=False)\n",
    "        # Convert to torch tensor\n",
    "        edge_index = torch.tensor(edges_np, dtype=torch.long)\n",
    "\n",
    "        # Define node features as one-hot encoded sequence\n",
    "        node_features = torch.Tensor(encoded_sequence)\n",
    "        ids = torch.arange(id_min, id_min+sequence_length, 1)\n",
    "\n",
    "        data = Data(x=node_features, edge_index=edge_index, ids=ids)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = self.parse_row(idx) \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9ff8793",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T00:44:56.229240Z",
     "iopub.status.busy": "2023-11-04T00:44:56.228465Z",
     "iopub.status.idle": "2023-11-04T00:44:57.076204Z",
     "shell.execute_reply": "2023-11-04T00:44:57.075396Z"
    },
    "papermill": {
     "duration": 3.413426,
     "end_time": "2023-11-04T00:44:57.078403",
     "exception": false,
     "start_time": "2023-11-04T00:44:53.664977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "infer_dataset = InferenceGraphDataset(parquet_name=TEST_PARQUET_FILE, edge_distance=EDGE_DISTANCE)\n",
    "infer_dataloader = DataLoader(infer_dataset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a20e278",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T00:45:02.263277Z",
     "iopub.status.busy": "2023-11-04T00:45:02.262894Z",
     "iopub.status.idle": "2023-11-04T00:45:02.270955Z",
     "shell.execute_reply": "2023-11-04T00:45:02.270156Z"
    },
    "papermill": {
     "duration": 2.56697,
     "end_time": "2023-11-04T00:45:02.272783",
     "exception": false,
     "start_time": "2023-11-04T00:44:59.705813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EdgeCNN(4, 1, num_layers=4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58e16f0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T00:45:07.442272Z",
     "iopub.status.busy": "2023-11-04T00:45:07.441877Z",
     "iopub.status.idle": "2023-11-04T02:24:40.383428Z",
     "shell.execute_reply": "2023-11-04T02:24:40.382028Z"
    },
    "papermill": {
     "duration": 5975.490906,
     "end_time": "2023-11-04T02:24:40.385628",
     "exception": false,
     "start_time": "2023-11-04T00:45:04.894722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10499 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch_geometric/warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "100%|██████████| 10499/10499 [1:39:32<00:00,  1.76it/s]\n"
     ]
    }
   ],
   "source": [
    "ids = np.empty(shape = (0,1),dtype=int)\n",
    "preds = np.empty(shape = (0,1),dtype=np.float32)\n",
    "\n",
    "\n",
    "for batch in tqdm(infer_dataloader):\n",
    "    batch = batch.to(device)\n",
    "    out = model(batch.x, batch.edge_index).detach().cpu().numpy()\n",
    "\n",
    "    ids = np.append(ids, batch.ids.detach().cpu().numpy())\n",
    "    preds = np.append(preds, out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535acec3",
   "metadata": {
    "papermill": {
     "duration": 3.350692,
     "end_time": "2023-11-04T02:24:47.071781",
     "exception": false,
     "start_time": "2023-11-04T02:24:43.721089",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission\n",
    "\n",
    "Create a csv file with the submission values. \n",
    "As you can see, I don't currently distinguish between `DMS_MaP` and `2A3_MaP`, so just write the same value to both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69d58efc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T02:24:53.756685Z",
     "iopub.status.busy": "2023-11-04T02:24:53.756267Z",
     "iopub.status.idle": "2023-11-04T02:24:55.739180Z",
     "shell.execute_reply": "2023-11-04T02:24:55.738065Z"
    },
    "papermill": {
     "duration": 5.318967,
     "end_time": "2023-11-04T02:24:55.741726",
     "exception": false,
     "start_time": "2023-11-04T02:24:50.422759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_df = pl.DataFrame({\"id\": ids, \"reactivity_DMS_MaP\": preds, \"reactivity_2A3_MaP\": preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "421ddc21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T02:25:02.381887Z",
     "iopub.status.busy": "2023-11-04T02:25:02.381535Z",
     "iopub.status.idle": "2023-11-04T02:25:55.112987Z",
     "shell.execute_reply": "2023-11-04T02:25:55.112178Z"
    },
    "papermill": {
     "duration": 56.069003,
     "end_time": "2023-11-04T02:25:55.115430",
     "exception": false,
     "start_time": "2023-11-04T02:24:59.046427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_df.write_csv(PRED_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1421e1ff",
   "metadata": {
    "papermill": {
     "duration": 3.447871,
     "end_time": "2023-11-04T02:26:01.979867",
     "exception": false,
     "start_time": "2023-11-04T02:25:58.531996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This is a basic GNN \"starter kit\" that does the basics. \n",
    "There are many things that can be improved, but I hope this helps people get started. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9831.310027,
   "end_time": "2023-11-04T02:26:08.591649",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-03T23:42:17.281622",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
